# PyFlink 从入门到精通

[toc]

Flink 是目前非常火热的流处理框架，可以很好地实现批流一体，即一套代码即可以用于批处理，也可以用于流处理。

官方文档很好地解释了 Flink 的基础概念和工作原理，但是对于 Python 开发者而言少了很多例子，很难快速基于 PyFlink 进行开发。

本文档基于常见的业务场景，提供了一些案例，来帮助小伙伴们快速上手 PyFlink。

在上手之前，先大致介绍一下 PyFlink：
1. PyFlink 是 Flink 对 Java API 的一层封装，运行时会启动 JVM 来与 Python 进行通信。
2. Flink 提供了多种不同层级的 API，层级越高封装程度越高（建议在 SQL API 或 Table API 进行编程），层级由高到低分别为:
    1) SQL API
    2) Table API
    3) DataStream API / DataSet API
    4) Stateful Streaming Processing
3. Flink 在做批处理的时候，是将批数据当成特殊的流数据来处理。

本教程的建议使用步骤是：
1. 按顺序实践。
1. 阅读每个案例的代码，理解每个部分的作用。
1. 阅读每个案例的代码头部文档里提到的扩展阅读，加深理解。

## 1、批处理 Word Count

该案例展示了如何用 Flink 做批处理，统计指定文件下的单词数，并将统计结果写入到新的文件下。

```bash
sh run.sh examples/1_word_count/batch.py
```

运行后的结果写到了同级目录下的 result.csv 中。

```
flink,3
pyflink,2
```

通过本案例，可以学到：
1. 如何创建批处理环境。
2. 如何创建数据源表和结果表，实现有单一流入和单一流出的 dataflow 的处理。
3. 如何用 Table API 和 SQL API 实现聚合逻辑。

### 2、自定义函数 UDF

该案例展示了如何用 Flink 管理自定义函数 UDF，来实现复杂的处理逻辑。

通过本案例，可以学到：
1. 如何创建并注册 UDF 。
2. 如何使用标量 UDF 和表值 UDF。

### 3、无状态流处理

该案例展示了如何用 Flink 进行无状态的流处理，来实现数据的实时同步和计算。
* 业务场景1：实时数仓同步，监听 MySQL 的数据变更，并实时同步到 HDFS。
* 业务场景2：对于 kafka 里的 json 格式的线上日志，来一条解析一条（不考虑上下文的关系），并将解析结果写入 kafka。
* 业务场景3：对于海量日志，逐条解析会造成大量的吞吐，性能下降，考虑加入缓存。

通过本案例，可以学到：
1. 如何创建流处理环境。
2. 如何使用各类 connector ，以及如何管理 connector 所需的依赖。
3. 如何实现实时数仓的数据同步。
4. 如何将数据同步到多个数据源。
4. 如何对实时同步进行优化。

### 4、有状态流处理

该案例展示了如何用 Flink 进行有状态的流处理，通过窗口函数完成统计需求。
* 业务场景1：考虑日志上下文，按不同用户 ID 分别统计各自每分钟的点击量。

通过本案例，可以学到：
1. 如何使用窗口函数。

### 5、多流 join

该案例展示了如何用 Flink 对多个数据源进行 join 处理。

业务场景：在做 AI 算法的在线学习时，可能存在多个数据源，需要同时对多个数据源的数据进行处理并生成所需特征。

通过本案例，可以学到：
1. 如何指定多个 source 。
2. 进行多流 join。
